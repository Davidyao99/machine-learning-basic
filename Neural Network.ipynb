{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Activation function for all nodes uses the following sigmoid function </h2>\n",
    "<h1> $$ \\frac{1}{(1 + e^{-\\theta^\\intercal X})} $$ </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(layer_input):\n",
    "    return 1 / (1 + np.exp(-(layer_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_row(layer_output):\n",
    "    if (layer_output.ndim == 1):\n",
    "        layer_output = layer_output.reshape(1,len(layer_output)) # single dimension will lead to error\n",
    "    ones = np.ones((1,layer_output.shape[1]), dtype = float) # adding in 1s for bias weight\n",
    "    layer_output = np.append(ones, layer_output, axis=0)\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(weights, X):\n",
    "    n = len(weights) # number of layers - 1, also number of iterations for foward propagation\n",
    "    layer_outputs = []\n",
    "    layer_output = X.transpose() # makes a copy as we need to add 1 to it to form first layer output\n",
    "    for i in range(n):\n",
    "        layer_output = add_ones_row(layer_output) # adding column of 1 so bias term for each layer is included\n",
    "        layer_outputs.append(layer_output) # important to include the 1 in as it is used for calculation in back propagation\n",
    "        layer_weight = weights[i]\n",
    "        layer_input = np.matmul(layer_weight, layer_output) # calculating z, input to next layer\n",
    "        layer_output = sigmoid(layer_input) # calculating z, output of next layer\n",
    "    layer_outputs.append(layer_output)\n",
    "    return layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(final_output, weights, y, lbd):\n",
    "    m = final_output.shape[1]\n",
    "    sum_squared_weights = sum(list(map(lambda x: (x**2).sum(),weights))) # sum of squared weights\n",
    "    regularization_cost = (lbd/(2*m))*(sum_squared_weights)\n",
    "    cost = (1/m)*(-y*np.log(final_output)-(1-y)*np.log(1-final_output)).sum() + regularization_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(weights, layer_outputs, y):\n",
    "    n = len(weights) # number of layers - 1, also number of iterations for foward propagation\n",
    "    gradients = []\n",
    "    layer_error = None;\n",
    "    for i in range(n,0,-1): # from n to 1 inclusive\n",
    "        if (i == n):\n",
    "            layer_error = layer_outputs[i] - y # final layer output - y\n",
    "        else:    \n",
    "            layer_error = np.matmul(weights[i].transpose(),layer_error)*layer_outputs[i]*(1-layer_outputs[i]) # vectorized implementation for layer error calculation\n",
    "            layer_error =  np.delete(layer_error, (0), axis=0) # remove first row which correspond to bias node\n",
    "        if (layer_error.ndim == 1):\n",
    "            layer_error = layer_error.reshape(1,len(layer_error)) # reshape from 1d to 2d so no error occurs\n",
    "        grad = np.matmul(layer_error,layer_outputs[i-1].transpose())\n",
    "        gradients.insert(0,grad)\n",
    "    return gradients\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, y, network_structure, iterations, alpha, lbd):\n",
    "    weights=[]\n",
    "    train_costs = []\n",
    "    m = X.shape[0]\n",
    "    for i in range(len(network_structure)-1): # intializing weights\n",
    "        layer_weights = np.random.rand(network_structure[i+1],network_structure[i]+1) # +1 for bias\n",
    "        if (layer_weights.ndim == 1):\n",
    "            layer_weights = layer_weights.reshape(1,len(layer_weights))\n",
    "        weights.append(layer_weights)\n",
    "    print(weights)    \n",
    "    for i in range(iterations):\n",
    "        layer_outputs = forward_propagation(weights, X) # forward propagation to calculate output of nodes at each layer\n",
    "        iter_cost = calculate_cost(layer_outputs[-1], weights, y, lbd)\n",
    "        train_costs.append(iter_cost)\n",
    "        gradients = back_propagation(weights, layer_outputs, y) # back progagation to calculate gradient of nodes at each layer\n",
    "        for j in range(len(weights)):\n",
    "            weights[j] -= (alpha/m)*(gradients[j]) + (lbd/m)*(weights[j])    \n",
    "    x_graph = np.arange(0,iterations,1);    \n",
    "    plt.plot(x_graph,train_costs, label='train') \n",
    "    plt.legend()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, mean, std):\n",
    "    return (X-mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, weights, threshold):\n",
    "    layer_outputs = forward_propagation(weights, X)\n",
    "    pred_result = layer_outputs[-1]\n",
    "    print(pred_result)\n",
    "    pred_result = (pred_result>=threshold).astype(int) # those above threshold = 1, 0 otherwise\n",
    "    return pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(X, y, network_structure, lbd):\n",
    "    grads_test = []\n",
    "    epsilon = 0.0001\n",
    "    weights = []\n",
    "    m=X.shape[0]\n",
    "    \n",
    "    for i in range(len(network_structure)-1): # intializing weights\n",
    "        layer_weights = np.random.rand(network_structure[i+1],network_structure[i]+1) # +1 for bias\n",
    "        if (layer_weights.ndim == 1):\n",
    "            layer_weights = layer_weights.reshape(1,len(layer_weights))\n",
    "        weights.append(layer_weights)\n",
    "        \n",
    "    for layer_weight in weights:\n",
    "        rows = layer_weight.shape[0]\n",
    "        cols = layer_weight.shape[1]\n",
    "        grad = np.zeros((rows ,cols))\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                layer_weight[j][k] += epsilon\n",
    "                layer_output_1 = forward_propagation(weights, X)\n",
    "\n",
    "                cost_1 = calculate_cost(layer_output_1[-1], weights, y, lbd)\n",
    "                layer_weight[j][k] -= 2*epsilon\n",
    "                layer_output_2 = forward_propagation(weights, X)\n",
    "                cost_2 = calculate_cost(layer_output_2[-1], weights, y, lbd)\n",
    "                layer_weight[j][k] += epsilon\n",
    "                \n",
    "                grad[j][k] = (cost_1 - cost_2) / (2*epsilon)\n",
    "        grads_test.append(grad)\n",
    "        \n",
    "    layer_outputs = forward_propagation(weights, X) # forward propagation to calculate output of nodes at each layer\n",
    "    iter_cost = calculate_cost(layer_outputs[-1], weights, y, lbd)\n",
    "    grads_train = back_propagation(weights, layer_outputs, y) # back progagation to calculate gradient of nodes at each layer\n",
    "    \n",
    "    total_diff = 0\n",
    "    num_weights = 0\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        grads_train[i] = (1/m)*(grads_train[i]) + (lbd/m)*(weights[i]) # calculating gradient with partial derivative found from back propagation\n",
    "        diff = np.abs(grads_train[i] - grads_test[i]).sum() # finding sum of absolute difference between test and train gradients \n",
    "        total_diff += diff\n",
    "        num_weights += grads_train[i].shape[0] * grads_train[i].shape[1]\n",
    "        \n",
    "    print(grads_train)\n",
    "    print(grads_test)\n",
    "    print(total_diff / num_weights) # calculate average difference of gradience per weight (should be very small, scale of x10^(-9))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 456.00484405   41.69414893 -296.76259797 -264.85673196  -20.12656321\n",
      " -320.90377055  123.95702882  144.6472803   -24.83174037  196.23993966\n",
      "  -50.32355832  435.5283476   276.18055562 -222.52748868  470.60632769\n",
      "   82.58270861  252.48534744 -286.10226999 -303.32572074 -147.28351107\n",
      "  233.51048781   16.59756636  120.05805374 -220.1151115    30.63007995\n",
      " -327.04026667    3.14327136  103.12217146 -531.07793418   28.99652563\n",
      "   12.09163352 -224.00305067  -64.05863138   82.6354104   293.6778142\n",
      "  476.29314172   95.23801196   94.11638787  116.33829871 -249.23765197\n",
      "  121.76199341  -43.84048693  -70.80988984  174.85710571 -108.18645448\n",
      " -118.19945214   60.04083634  -34.48822927 -147.96754431  160.59694715\n",
      " -308.70112075  200.148303     83.27682528   -3.82995516   66.83508563\n",
      " -156.89843686  -90.19367443    7.21068951  116.92523008 -190.78489104\n",
      "    4.30143118 -469.85459011   34.42009584  398.75178174    6.46966836\n",
      "  106.80831669  234.76173371  177.51136586  190.18291207  141.18511973\n",
      " -296.96354322   13.16795652  -53.1227786    73.48781825   49.77793322\n",
      "  527.54757838  412.37648097  386.80946138  331.49005286 -166.43402636\n",
      "  -91.92437736   57.9496071   136.26551755  201.08811377 -196.54158642\n",
      " -112.58914081 -333.55265331   -6.27417959  -76.634447    -59.8546172\n",
      " -221.87575808  332.56379196 -106.67231466  147.06237946  184.67200556\n",
      "  341.24386153  123.44057933  284.87615487 -139.17767763  104.38601572\n",
      "  194.85177285 -178.98452112 -186.99652531   15.03838946   94.69579089\n",
      "  -68.52138062 -100.068891   -126.38952902 -215.81050579  -36.24207478\n",
      " -234.61907245  -57.52690556  370.50703542  -91.40072688 -292.80671689\n",
      "   91.74593364   38.0725707   174.5275562   130.24733885   64.51703147\n",
      "  134.84422256 -186.68043415 -161.72949045   -3.16915471  239.25617336\n",
      "  250.79941827 -214.50277535 -151.61622423 -125.06266145 -144.4564707\n",
      " -235.29203848   30.70376981   10.24681242  410.52541955  236.78135766\n",
      "    7.99157529  101.94967525 -330.71707961   19.05266685 -379.14107492\n",
      "  -49.70704798 -181.62671564   58.53792208  119.958201    183.38731852\n",
      "  178.15194439 -125.00936227 -341.76372494 -272.73876274 -313.53629557\n",
      "  353.25023995 -262.17351423 -333.9155192   155.91917641   46.09431403\n",
      "   17.30925797 -133.76386762 -198.52372505  332.26511772  189.41493599\n",
      "  236.93170798 -163.40897491  -96.61107147 -283.4483843  -504.39875704\n",
      "  459.8177562  -223.00567612  -34.95551996  182.06858343   53.44035373\n",
      "  212.11095445 -107.51200905  -92.65295172 -390.51894308  351.94432109\n",
      " -629.0738424  -236.12459261  -99.99252209  168.9616289    19.72010975\n",
      "  183.30465969  273.37630008  150.30934838   47.06804111 -214.76802194\n",
      " -165.40809535    8.44012242  -21.49087669 -121.33840019 -334.69105541\n",
      "  245.41777155   30.62447578   92.0825215   212.47497203  -65.20669135\n",
      "  126.95711105   72.56568955 -140.49209134  373.86142035  381.35670008\n",
      "  557.48776285  160.20265372  109.24599752  298.3856232  -210.9543557\n",
      " -305.25981151  554.87091081 -143.07403932  259.99672318 -269.95428545\n",
      "  342.6221874   341.42162359  354.29727005 -352.35131229 -192.57793161\n",
      "  469.51394663   43.06663257  358.64037719  -43.04327783 -325.53888465\n",
      "   98.53092743  476.82941766 -179.46557195 -205.68197285  125.69202906\n",
      " -492.7070524  -194.51396655  116.26238481 -145.94206289   14.44517645\n",
      "    2.27362736 -213.50715941 -182.73631069   50.93414245  259.44964556\n",
      "  103.9784071   217.89838528 -189.23338486  307.39190498  133.8691156\n",
      " -128.74517007 -242.8677346    66.42986615   80.33921309  321.7933877\n",
      "  708.60521035 -259.11386178  -40.166063    112.46248741   65.33608434\n",
      "  213.86848698  482.49372081  227.84091127  -63.62122924  297.15318992\n",
      " -507.68668628 -145.4978864  -125.98299864  154.17914081 -147.97648196\n",
      "   69.07798272   10.81811381 -633.61440102   88.85471848  338.0267989\n",
      "  127.85831828  194.24821978  -30.33968196  294.14504416  165.75978089\n",
      " -252.0126326   -21.68811734  595.45241082   22.53857325 -104.63569424\n",
      "  286.23146376 -149.84133152 -252.17434771 -337.33584407  107.4158281\n",
      " -189.01233631  241.70205537  136.67842724   20.67024865   45.65251318\n",
      " -201.23910822 -114.78424874 -103.67516245  557.21929935  -59.29133022\n",
      " -307.328257     98.55904723  263.07203031   79.2353643     7.09447717\n",
      "  372.01443194  148.12715739   47.28004741   80.31787222  351.21806993\n",
      "  284.11004675  295.88148999 -221.2564582   137.42457862 -188.4709306\n",
      "  -12.8049498   229.74319494   13.49158169 -466.50441603 -261.24789759\n",
      "  291.69038817   23.56836483  585.78809746  148.26603254  756.03067314\n",
      " -219.49279594  188.19508022   52.20696382  221.95504341  164.73091133\n",
      "  -63.96764731 -278.68931481 -123.14644027  363.15360561    2.83276596\n",
      "  376.53677061  386.55747014   54.1525533     6.28745434  -28.46717961\n",
      "  126.52026967  471.9110205   -35.8585573    50.9269756    -8.88487127\n",
      " -379.56499491   29.55886835 -285.0968739  -389.46166477   76.28328109\n",
      "  129.1672815   -32.76811866 -273.95123943 -389.57856705 -464.4134215\n",
      "   27.77721224  206.03727035   -6.73338185 -137.46391519  304.25328626\n",
      "  -60.98012711 -199.47090465   33.36178924 -114.77946084   44.75591876\n",
      " -132.25225383   68.72233494  199.24979662 -419.17418161   60.31184125\n",
      "  -63.49177366    8.24463942 -155.30709933   11.74602541  -44.65625223\n",
      "  249.13111746  352.07679588  453.43802793  298.9552238   332.4926643\n",
      " -140.03884622   25.69272256  186.74879047  -39.54327815  211.69447894\n",
      " -556.88199803  -81.43032605 -155.94738888   60.40650667 -559.98355526\n",
      "  -94.28338781 -739.59198312  331.76849035 -737.24425529   25.6251119\n",
      " -101.07987368 -255.62468559 -427.69552391 -269.58863397  250.58373258\n",
      "  229.51109392 -186.72259795 -202.48435293 -154.67731097  242.0088309\n",
      "    8.25612818   80.03321088  171.82503065  -93.52950517  151.99896486\n",
      "   46.91228498  172.00189073    1.36369706  -75.18123008 -355.13313786\n",
      "  -77.60846384   83.15849154  189.3288403   169.49990294  421.37061471\n",
      "  -84.00014603  -35.87125361 -288.39411523  -87.86606517  161.74634443\n",
      "  242.08279121 -252.06720901  -68.70287534  -20.33821594 -306.950669\n",
      " -124.93534521  117.1125249   -83.95187434 -147.91250085   31.96693621\n",
      "  747.21875544 -298.03756264 -103.13614965 -308.22596233  120.49744667\n",
      "  208.20660519 -213.20556589  -56.86367095   15.50262577 -193.88994119\n",
      "  136.42490084   52.03112682 -262.93166796 -382.39553622  191.51548019\n",
      "  -56.61767787    5.79179061   81.54730296 -596.18943141  377.22231357\n",
      "  199.00340778 -380.22392024  652.16983478   76.92135917  -24.10623999\n",
      " -386.77791682  330.79435949   93.83407448   31.60702388   53.63618177\n",
      "  -98.15988264  433.17722587 -190.6145916   274.93548516  -39.7865867\n",
      "   28.91419545  438.01399657 -209.16846308   57.97722073  -64.10345529\n",
      " -333.40873576  273.19579522  443.59825106  206.30257649   47.60049635\n",
      " -408.12825643  204.23601667  116.42772105  242.11588404 -254.64001728\n",
      " -102.11433317 -252.32797148 -302.50188053   -3.3421548  -127.25311485\n",
      "  126.32742579 -451.12662251  166.37964639 -419.49580668   17.11189325\n",
      "  267.72523909  229.91704326   27.25540628 -351.89996593  -62.92715343\n",
      "  239.27279138  121.82869264 -261.62187435  468.78390558  461.01770251\n",
      "  185.09803845 -313.12872648   30.94997428 -257.91451755 -167.91837703]\n",
      "---X---\n",
      "[[  4.41227487  -3.30870152  24.30771187  -2.5209213 ]\n",
      " [  1.09609842  15.82481117  -9.09232405  -5.91636658]\n",
      " [  1.87603226  -3.29869958 -11.92764612  -2.04876511]\n",
      " ...\n",
      " [  0.75827051  -2.21364703   2.67832244   6.31006591]\n",
      " [  7.03964503 -10.07493118  -6.18180343  13.91913458]\n",
      " [  6.20613481   7.042333   -11.85734368 -35.07474618]]\n",
      "---beta---\n",
      "[ 0.74679114  4.60080908 -4.1237795   0.58185694]\n",
      "---y---\n",
      "[1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0\n",
      " 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
      " 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1\n",
      " 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1\n",
      " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0\n",
      " 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
      " 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0\n",
      " 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1\n",
      " 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "X=np.random.normal(size=(500,4)) * 10\n",
    "beta1 = np.random.normal(size=(4,)) * 2\n",
    "beta2 = np.random.normal(size=(4,)) * 3\n",
    "y = X @ (beta1 ** 2) + X @ beta2 + np.random.normal(size=(500,))\n",
    "print(y)\n",
    "y = (y > 0).astype(int)\n",
    "train_x = X[:400, :]\n",
    "train_y = y[:400]\n",
    "test_x = X[:400, :]\n",
    "test_y = y[:400]\n",
    "print(\"---X---\")\n",
    "print(X)\n",
    "print(\"---beta---\")\n",
    "print(beta)\n",
    "print(\"---y---\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.47354236, 0.32733302, 0.76215499, 0.84269386, 0.1212724 ],\n",
      "       [0.35235551, 0.51444852, 0.03779016, 0.3721308 , 0.83963233],\n",
      "       [0.62835116, 0.10670393, 0.46250696, 0.66336644, 0.53577632],\n",
      "       [0.17113427, 0.46609931, 0.96290914, 0.67226535, 0.88164363]]), array([[0.76741982, 0.54822342, 0.60900636, 0.63032623, 0.51965283]])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = neural_network(train_x, train_y, [train_x.shape[1],4,1], 20000, 0.01, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99336706 0.99482681 0.01979782 0.02052246 0.05591755 0.0205223\n",
      "  0.9935372  0.99604255 0.04301922 0.99335389 0.0151601  0.99335496\n",
      "  0.99349947 0.01303823 0.9939611  0.99606432 0.9933709  0.02050975\n",
      "  0.02018868 0.01279198 0.99335389 0.966315   0.9960962  0.01212339\n",
      "  0.98882201 0.01210929 0.88803965 0.99609552 0.02052187 0.99044009\n",
      "  0.94328941 0.02052261 0.02144628 0.99331262 0.99335389 0.99335748\n",
      "  0.99609112 0.99363564 0.99538589 0.01210284 0.99498062 0.01888435\n",
      "  0.02093723 0.99335398 0.02035308 0.01224886 0.99585343 0.02272988\n",
      "  0.01210345 0.9933554  0.02052256 0.99337445 0.99606507 0.48657689\n",
      "  0.99597151 0.01210315 0.02052047 0.88186202 0.99579013 0.02052263\n",
      "  0.8123168  0.0205136  0.99023433 0.9933539  0.89509809 0.99501735\n",
      "  0.99335439 0.99337418 0.99610325 0.99533544 0.01210293 0.96322191\n",
      "  0.02276539 0.99602198 0.99283988 0.99335532 0.99610326 0.99609908\n",
      "  0.99352641 0.01212415 0.01291108 0.99580898 0.99602935 0.99376048\n",
      "  0.02052125 0.01918164 0.01212648 0.36924475 0.01233456 0.02152543\n",
      "  0.01210437 0.99610257 0.0205443  0.99335362 0.99610325 0.99335437\n",
      "  0.9933524  0.99610326 0.01211109 0.9933473  0.99610259 0.02051639\n",
      "  0.02052016 0.96104392 0.99570185 0.01261795 0.01214298 0.01560476\n",
      "  0.01210274 0.03307916 0.01210281 0.01439508 0.99608403 0.01738628\n",
      "  0.02044654 0.99608122 0.99438489 0.9961018  0.99335295 0.9959515\n",
      "  0.99335372 0.01908795 0.02052273 0.30444139 0.99335833 0.99335578\n",
      "  0.01210278 0.01211439 0.01244455 0.01406672 0.01220812 0.99151431\n",
      "  0.94379154 0.99335389 0.99335389 0.88324746 0.99334549 0.02052202\n",
      "  0.97222368 0.01219396 0.02342209 0.01210278 0.99580768 0.99610107\n",
      "  0.99420679 0.99335387 0.01473307 0.02052261 0.0205226  0.01210354\n",
      "  0.99610326 0.02052261 0.01870696 0.99335385 0.99227607 0.97346347\n",
      "  0.01213217 0.01279669 0.99423839 0.99610201 0.99345406 0.01210731\n",
      "  0.02057714 0.01860679 0.02049651 0.99335961 0.01210274 0.02239792\n",
      "  0.99444622 0.99550483 0.99335389 0.02053737 0.01217603 0.02052261\n",
      "  0.99335389 0.0205226  0.01226021 0.0125979  0.99385218 0.98045712\n",
      "  0.99335514 0.99348513 0.99335954 0.99530654 0.01261237 0.01803171\n",
      "  0.87945694 0.05583433 0.01225435 0.01210276 0.99549693 0.99184097\n",
      "  0.99358761 0.99610156 0.01289206 0.99335281 0.99599762 0.012181\n",
      "  0.99590521 0.99610275 0.99340849 0.99381697 0.99334924 0.9933539\n",
      "  0.01210276 0.01550139 0.99335389 0.01216319 0.99596826 0.02052261\n",
      "  0.99610317 0.99609525 0.99598853 0.01210796 0.01213072 0.99354474\n",
      "  0.99498138 0.99335389 0.02596309 0.012104   0.99362915 0.99340274\n",
      "  0.01887998 0.01811619 0.996069   0.02052261 0.02052008 0.99609358\n",
      "  0.01284521 0.96256598 0.78323201 0.02052217 0.02049552 0.99264466\n",
      "  0.99517146 0.99347291 0.99386524 0.01210987 0.9933785  0.99335385\n",
      "  0.0205056  0.01998967 0.99318789 0.99605211 0.9933539  0.99335389\n",
      "  0.01215929 0.02601966 0.99356814 0.99313253 0.9933887  0.99610326\n",
      "  0.99610326 0.02133755 0.99335389 0.01988618 0.01216047 0.01210586\n",
      "  0.99338644 0.01211976 0.99322952 0.94963697 0.02040608 0.9955068\n",
      "  0.99490939 0.99335333 0.99605542 0.02454371 0.99600041 0.99610295\n",
      "  0.02033485 0.07854393 0.99335562 0.98291701 0.01212203 0.99335389\n",
      "  0.02021464 0.01210529 0.01953952 0.99523164 0.02052263 0.99541053\n",
      "  0.99435913 0.98240427 0.992065   0.02052261 0.01211119 0.01213307\n",
      "  0.99610113 0.01295628 0.02025893 0.9939579  0.99610326 0.99604952\n",
      "  0.88314566 0.9933551  0.99335365 0.99539905 0.99605715 0.99335389\n",
      "  0.99606571 0.99610003 0.02051195 0.99335718 0.01218004 0.20173055\n",
      "  0.99335389 0.96019028 0.01635096 0.02052252 0.99610302 0.98271451\n",
      "  0.99335391 0.99416999 0.99338763 0.0205226  0.99335389 0.99266685\n",
      "  0.99370236 0.99610314 0.012815   0.01210995 0.0202539  0.99474577\n",
      "  0.85600437 0.99550968 0.99571942 0.9928123  0.89743804 0.03199859\n",
      "  0.99335251 0.99558442 0.03003896 0.99273235 0.32428856 0.02052261\n",
      "  0.99196106 0.0121028  0.02013417 0.99516851 0.99335278 0.02985258\n",
      "  0.01393245 0.01210274 0.02052258 0.99065445 0.99610326 0.41562071\n",
      "  0.01210943 0.99610311 0.02179791 0.0205226  0.99061374 0.01565044\n",
      "  0.99517647 0.01453387 0.99318162 0.99610019 0.02052204 0.99583613\n",
      "  0.02130685 0.92843928 0.0121035  0.94135901 0.01621026 0.99610326\n",
      "  0.99599674 0.99350201 0.99335391 0.9960888  0.0204696  0.98785795\n",
      "  0.99335388 0.01787938 0.99335431 0.020522   0.02071069 0.02051824\n",
      "  0.99307382 0.02052244 0.01916615 0.01233384 0.99340821 0.02052201\n",
      "  0.98913981 0.02056284 0.01798259 0.02052098 0.02052261 0.99606271\n",
      "  0.99600851 0.02052247 0.02038166 0.01210309 0.99610326 0.88365813\n",
      "  0.99328967 0.99554593 0.01215901 0.9933704 ]]\n",
      "---\n",
      "[1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0\n",
      " 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
      " 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1\n",
      " 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1\n",
      " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0\n",
      " 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
      " 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1]\n",
      "[1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0\n",
      " 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
      " 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1\n",
      " 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1\n",
      " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0\n",
      " 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
      " 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "pred_y = prediction(test_x, weights, 0.5)\n",
    "print(\"---\")\n",
    "pred_y = pred_y.ravel()\n",
    "result = (pred_y == test_y).astype(int)\n",
    "print(result.sum() / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
