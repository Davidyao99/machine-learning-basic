{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Activation function for all nodes uses the following sigmoid function </h2>\n",
    "<h1> $$ \\frac{1}{(1 + e^{-\\theta^\\intercal X})} $$ </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(layer_input):\n",
    "    return 1 / (1 + np.exp(-(layer_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_row(layer_output):\n",
    "    if (layer_output.ndim == 1):\n",
    "        layer_output = layer_output.reshape(1,len(layer_output)) # single dimension will lead to error\n",
    "    ones = np.ones((1,layer_output.shape[1]), dtype = float) # adding in 1s for bias weight\n",
    "    layer_output = np.append(ones, layer_output, axis=0)\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(weights, X):\n",
    "    n = len(weights) # number of layers - 1, also number of iterations for foward propagation\n",
    "    layer_outputs = []\n",
    "    layer_output = X.transpose() # makes a copy as we need to add 1 to it to form first layer output\n",
    "    for i in range(n):\n",
    "        layer_output = add_ones_row(layer_output) # adding column of 1 so bias term for each layer is included\n",
    "        layer_outputs.append(layer_output) # important to include the 1 in as it is used for calculation in back propagation\n",
    "        layer_weight = weights[i]\n",
    "        layer_input = np.matmul(layer_weight, layer_output) # calculating z, input to next layer\n",
    "        layer_output = sigmoid(layer_input) # calculating z, output of next layer\n",
    "    layer_outputs.append(layer_output)\n",
    "    return layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(final_output, weights, y, lbd):\n",
    "    m = len(y)\n",
    "    sum_squared_weights = sum(list(map(lambda x: (x**2).sum(),weights))) # sum of squared weights\n",
    "    regularization_cost = (lbd/(2*m))*(sum_squared_weights)\n",
    "    cost = (1/m)*(-y*np.log(final_output)-(1-y)*np.log(1-final_output)).sum() + regularization_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(weights, layer_outputs, y):\n",
    "    n = len(weights) # number of layers - 1, also number of iterations for foward propagation\n",
    "    gradients = []\n",
    "    layer_error = None;\n",
    "    for i in range(n,0,-1): # from n to 1 inclusive\n",
    "        \n",
    "        if (i == n):\n",
    "            layer_error = layer_outputs[i] - y # final layer output - y\n",
    "        else:    \n",
    "            layer_error = np.matmul(weights[i].transpose(),layer_error)*layer_outputs[i]*(1-layer_outputs[i]) # vectorized implementation for layer error calculation\n",
    "            layer_error =  np.delete(layer_error, (0), axis=0) # remove first row which correspond to bias node\n",
    "        if (layer_error.ndim == 1):\n",
    "            layer_error = layer_error.reshape(1,len(layer_error)) # reshape from 1d to 2d so no error occurs\n",
    "        grad = np.matmul(layer_error,layer_outputs[i-1].transpose())\n",
    "        gradients.insert(0,grad)\n",
    "    return gradients\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, y, network_structure, iterations, lbd):\n",
    "    weights=[]\n",
    "    train_costs = []\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for i in range(len(network_structure)-1): # intializing weights\n",
    "        layer_weights = np.random.rand(network_structure[i+1],network_structure[i]+1) # +1 for bias\n",
    "        if (layer_weights.ndim == 1):\n",
    "            layer_weights = layer_weights.reshape(1,len(layer_weights))\n",
    "        weights.append(layer_weights)\n",
    "    for i in range(iterations):\n",
    "        layer_outputs = forward_propagation(weights, X) # forward propagation to calculate output of nodes at each layer\n",
    "        iter_cost = calculate_cost(layer_outputs[-1], weights, y, lbd)\n",
    "        train_costs.append(iter_cost)\n",
    "        print(iter_cost)\n",
    "        gradients = back_propagation(weights, layer_outputs, y) # back progagation to calculate gradient of nodes at each layer\n",
    "#         weights -= (1/m)*(gradients) + (lbd/(m)*weights)\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] -= (1/m)*(gradients[i]) + (lbd/m)*(weights[i])\n",
    "    x_graph = np.arange(0,iterations,1);    \n",
    "    plt.plot(x_graph,train_costs, label='train') \n",
    "    plt.legend()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, mean, std):\n",
    "    return (X-mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, weights, threshold):\n",
    "    layer_outputs = forward_propagation(weights, X)\n",
    "    pred_result = layer_outputs[-1]\n",
    "    print(pred_result)\n",
    "    pred_result = (pred_result>=threshold).astype(int) # those above threshold = 1, 0 otherwise\n",
    "    return pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(X, y, network_structure, lbd):\n",
    "    grads_test = []\n",
    "    epsilon = 0.0001\n",
    "    weights = []\n",
    "    m=X.shape[0]\n",
    "    \n",
    "    for i in range(len(network_structure)-1): # intializing weights\n",
    "        layer_weights = np.random.rand(network_structure[i+1],network_structure[i]+1) # +1 for bias\n",
    "        if (layer_weights.ndim == 1):\n",
    "            layer_weights = layer_weights.reshape(1,len(layer_weights))\n",
    "        weights.append(layer_weights)\n",
    "        \n",
    "    for layer_weight in weights:\n",
    "        rows = layer_weight.shape[0]\n",
    "        cols = layer_weight.shape[1]\n",
    "        grad = np.zeros((rows ,cols))\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                layer_weight[j][k] += epsilon\n",
    "                layer_output_1 = forward_propagation(weights, X)\n",
    "                cost_1 = calculate_cost(layer_output_1[-1], weights, y, lbd)\n",
    "                layer_weight[j][k] -= 2*epsilon\n",
    "                layer_output_2 = forward_propagation(weights, X)\n",
    "                cost_2 = calculate_cost(layer_output_2[-1], weights, y, lbd)\n",
    "                layer_weight[j][k] += epsilon\n",
    "                \n",
    "                grad[j][k] = (cost_1 - cost_2) / (2*epsilon)\n",
    "        grads_test.append(grad)\n",
    "        \n",
    "    layer_outputs = forward_propagation(weights, X) # forward propagation to calculate output of nodes at each layer\n",
    "    iter_cost = calculate_cost(layer_outputs[-1], weights, y, lbd)\n",
    "    grads_train = back_propagation(weights, layer_outputs, y) # back progagation to calculate gradient of nodes at each layer\n",
    "    \n",
    "    total_diff = 0\n",
    "    num_weights = 0\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        grads_train[i] = (1/m)*(grads_train[i]) + (lbd/m)*(weights[i]) # calculating gradient with partial derivative found from back propagation\n",
    "        diff = np.abs(grads_train[i] - grads_test[i]).sum() # finding sum of absolute difference between test and train gradients \n",
    "        total_diff += diff\n",
    "        num_weights += grads_train[i].shape[0] * grads_train[i].shape[1]\n",
    "        \n",
    "    print( grads_train)\n",
    "    print(grads_test)\n",
    "    print(total_diff / num_weights) # calculate average difference of gradience per weight (should be very small, scale of x10^(-9))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.01166516, -0.00247763, -0.00524509,  0.00059663, -0.00021572,\n",
      "        -0.00187032, -0.00231421, -0.0019332 , -0.00294195],\n",
      "       [ 0.01286311, -0.00479028, -0.00600782, -0.00076764, -0.00086453,\n",
      "        -0.00191925, -0.00375501, -0.00244362, -0.00595076]])\n",
      " array([[0.04803706, 0.01805212, 0.02363361],\n",
      "       [0.06672815, 0.02243605, 0.03041693]])\n",
      " array([[0.47497123, 0.35381774, 0.38106319]])]\n",
      "[array([[ 0.01166516, -0.00247763, -0.00524509,  0.00059663, -0.00021572,\n",
      "        -0.00187032, -0.00231421, -0.0019332 , -0.00294195],\n",
      "       [ 0.01286311, -0.00479028, -0.00600782, -0.00076764, -0.00086453,\n",
      "        -0.00191925, -0.00375501, -0.00244362, -0.00595076]]), array([[0.04803706, 0.01805212, 0.02363361],\n",
      "       [0.06672815, 0.02243605, 0.03041693]]), array([[0.47497123, 0.35381774, 0.38106319]])]\n",
      "1.4382679766174446e-11\n"
     ]
    }
   ],
   "source": [
    "gradient_checking(train_x, train_y, [train_x.shape[1],2,2,1], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[array([[ 7.63954659e-03,  8.00829071e-04, -2.38528441e-04,\n",
      "        -3.29355804e-04, -3.28746036e-05, -8.47114038e-05,\n",
      "        -4.49042339e-04,  3.49670531e-05,  1.15724659e-04,\n",
      "        -3.84202885e-04, -4.79987530e-04,  3.43588422e-05,\n",
      "         1.73651912e-04,  1.94011387e-04,  4.42050855e-03,\n",
      "        -7.06221083e-04, -7.71651933e-04,  1.36500110e-04,\n",
      "         1.90031383e-04, -5.60291631e-04, -6.19309551e-04],\n",
      "       [ 7.48343622e-03,  1.05609817e-03, -3.52583578e-04,\n",
      "        -4.13812403e-04, -1.03218479e-04,  2.50140702e-04,\n",
      "        -7.14856506e-04, -1.88085323e-04, -2.55724860e-04,\n",
      "        -3.74612875e-04, -9.18920218e-04,  3.55315932e-04,\n",
      "        -3.71409881e-04, -3.64158504e-04,  4.05549991e-03,\n",
      "        -1.53387368e-03, -1.79273443e-03,  3.31140017e-04,\n",
      "        -3.40315574e-04, -9.35795902e-04, -4.92711968e-04]]), array([[0.08643012, 0.04464805, 0.05039231],\n",
      "       [0.01418482, 0.00713188, 0.00822809]]), array([[0.53571598, 0.41439926, 0.40873392]])]\n",
      "[array([[ 7.63954659e-03,  8.00829072e-04, -2.38528441e-04,\n",
      "        -3.29355804e-04, -3.28746042e-05, -8.47114046e-05,\n",
      "        -4.49042338e-04,  3.49670537e-05,  1.15724660e-04,\n",
      "        -3.84202884e-04, -4.79987529e-04,  3.43588435e-05,\n",
      "         1.73651912e-04,  1.94011389e-04,  4.42050855e-03,\n",
      "        -7.06221081e-04, -7.71651933e-04,  1.36500109e-04,\n",
      "         1.90031384e-04, -5.60291631e-04, -6.19309550e-04],\n",
      "       [ 7.48343622e-03,  1.05609817e-03, -3.52583581e-04,\n",
      "        -4.13812401e-04, -1.03218479e-04,  2.50140700e-04,\n",
      "        -7.14856505e-04, -1.88085323e-04, -2.55724860e-04,\n",
      "        -3.74612875e-04, -9.18920218e-04,  3.55315931e-04,\n",
      "        -3.71409880e-04, -3.64158504e-04,  4.05549990e-03,\n",
      "        -1.53387368e-03, -1.79273443e-03,  3.31140017e-04,\n",
      "        -3.40315572e-04, -9.35795902e-04, -4.92711966e-04]]), array([[0.08643012, 0.04464805, 0.05039231],\n",
      "       [0.01418482, 0.00713188, 0.00822809]]), array([[0.53571598, 0.41439926, 0.40873392]])]\n",
      "7.2132199624988035e-12\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/mobile_price_classification_train.csv\")\n",
    "train_x = df.loc[:, df.columns != \"price_range\"]\n",
    "train_y = df[\"price_range\"]\n",
    "train_mean = train_x.mean(axis=0) # mean normalization\n",
    "train_std = train_x.std(axis=0)\n",
    "train_x = normalize(train_x,train_mean ,train_std).values\n",
    "train_y = train_y.values\n",
    "n_values = np.max(train_y) + 1\n",
    "train_y = np.eye(n_values)[train_y]\n",
    "print(train_y)\n",
    "train_y = train_y[:,0]\n",
    "print(train_y)\n",
    "gradient_checking(train_x, train_y.transpose(), [train_x.shape[1],2,2,1], 0.1)\n",
    "# weights = neural_network(train_x, train_y, [train_x.shape[1],4,4,1], 100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34243646 0.19436678 0.19526518 ... 0.19367267 0.24670362 0.18682309]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = prediction(train_x, weights, 0.5)\n",
    "pred_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 5, 6, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[3,4,5,6],[1,2]]\n",
    "for i in a:\n",
    "    i.append(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n",
      "268\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/logistic_diabetes.csv\")\n",
    "print(df.isnull().sum())\n",
    "print(len(df[df['Outcome']==1]))\n",
    "print(len(df[df['Outcome']==0])) # making sure data is balanced, a bit of imbalance but should be okay\n",
    "train=df.sample(frac=0.75,random_state=150) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "\n",
    "train_x = train.loc[:,train.columns != \"Outcome\"] # splitting dependent and independent variables\n",
    "test_x = test.loc[:,test.columns != \"Outcome\"]\n",
    "train_y = train['Outcome'].values\n",
    "test_y = test['Outcome'].values\n",
    "\n",
    "train_mean = train_x.mean(axis=0) # mean normalization\n",
    "train_std = train_x.std(axis=0)\n",
    "train_x = normalize(train_x,train_mean ,train_std)\n",
    "test_x = normalize(test_x,train_mean ,train_std )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4728616137640278\n",
      "0.7280632224402533\n",
      "0.6663536321764878\n",
      "0.6652122317386844\n",
      "0.6646611731247231\n",
      "0.6641347001252574\n",
      "0.6636209355579398\n",
      "0.663119018176267\n",
      "0.662628353940941\n",
      "0.6621483819772243\n",
      "0.6616785680472085\n",
      "0.6612184027397903\n",
      "0.6607673998264444\n",
      "0.6603250947264512\n",
      "0.6598910430784128\n",
      "0.659464819410365\n",
      "0.6590460159005682\n",
      "0.6586342412216313\n",
      "0.6582291194612573\n",
      "0.6578302891134871\n",
      "0.6574374021348474\n",
      "0.6570501230602958\n",
      "0.6566681281742895\n",
      "0.6562911047326985\n",
      "0.6559187502316487\n",
      "0.655550771719702\n",
      "0.6551868851500767\n",
      "0.654826814769885\n",
      "0.654470292543607\n",
      "0.6541170576082462\n",
      "0.653766855757818\n",
      "0.6534194389550045\n",
      "0.6530745648679871\n",
      "0.6527319964306233\n",
      "0.6523915014242735\n",
      "0.6520528520797257\n",
      "0.651715824697779\n",
      "0.6513801992871644\n",
      "0.6510457592185885\n",
      "0.6507122908937721\n",
      "0.6503795834284587\n",
      "0.6500474283484399\n",
      "0.6497156192977296\n",
      "0.6493839517580914\n",
      "0.6490522227791863\n",
      "0.6487202307186829\n",
      "0.6483877749917214\n",
      "0.648054655829193\n",
      "0.6477206740443427\n",
      "0.6473856308072599\n",
      "0.6470493274268753\n",
      "0.6467115651401293\n",
      "0.6463721449080233\n",
      "0.6460308672183211\n",
      "0.6456875318947024\n",
      "0.6453419379122277\n",
      "0.6449938832190116\n",
      "0.6446431645640509\n",
      "0.6442895773311974\n",
      "0.6439329153793136\n",
      "0.6435729708886934\n",
      "0.6432095342138747\n",
      "0.6428423937430255\n",
      "0.6424713357641225\n",
      "0.6420961443381998\n",
      "0.6417166011799884\n",
      "0.6413324855463186\n",
      "0.6409435741327125\n",
      "0.6405496409786429\n",
      "0.6401504573819844\n",
      "0.6397457918232451\n",
      "0.6393354099002109\n",
      "0.6389190742736959\n",
      "0.6384965446251452\n",
      "0.6380675776268845\n",
      "0.6376319269258732\n",
      "0.6371893431418633\n",
      "0.6367395738809163\n",
      "0.6362823637652839\n",
      "0.6358174544806953\n",
      "0.6353445848421408\n",
      "0.6348634908792785\n",
      "0.6343739059426117\n",
      "0.6338755608316248\n",
      "0.6333681839460682\n",
      "0.6328515014615941\n",
      "0.6323252375309454\n",
      "0.6317891145118819\n",
      "0.6312428532229973\n",
      "0.6306861732285456\n",
      "0.6301187931533262\n",
      "0.629540431028612\n",
      "0.6289508046699981\n",
      "0.6283496320879381\n",
      "0.627736631931599\n",
      "0.6271115239664954\n",
      "0.6264740295861899\n",
      "0.6258238723581266\n",
      "0.6251607786034347\n",
      "0.6244844780102796\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = neural_network(train_x, train_y, [train_x.shape[1],4,4,1], 100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00804275,  0.19191353,  0.97733366,  0.20716204,  0.10262259,\n",
       "          0.28133791,  0.576963  ,  0.34925585,  0.78804953],\n",
       "        [ 0.04989449,  0.14346172,  0.30704458,  0.54846342,  0.25884289,\n",
       "          0.93435235,  0.19291091,  0.51723296,  0.35379309],\n",
       "        [ 0.03722254,  0.26463515,  0.71325038,  0.77420797,  0.69432845,\n",
       "          0.94143741,  0.86620388,  0.60261677,  0.12111132],\n",
       "        [ 0.71934174,  0.16570095,  0.14212012,  0.66468094,  0.06677443,\n",
       "          0.8671216 ,  0.42862647,  0.74277609,  0.87640519]]),\n",
       " array([[ 0.24627008,  0.84522647,  0.55121489,  0.54061708,  0.44709743],\n",
       "        [-0.16446839,  0.77007439,  0.89984901,  0.92640136,  0.76842616],\n",
       "        [ 1.01700767, -0.24241192,  0.06414718,  0.60195351,  0.12884121],\n",
       "        [ 0.89030838,  0.32302372,  0.02847392,  0.22726528,  0.50558045]]),\n",
       " array([[-1.05025378,  0.45211963,  0.77026177, -0.5038086 , -0.11555081]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510416666666666\n"
     ]
    }
   ],
   "source": [
    "pred_y = prediction(train_x, weights, 0.5)\n",
    "pred_y = pred_y.flatten()\n",
    "result = pred_y == train_y\n",
    "print(sum(result) / len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
