{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Activation function for all nodes uses the following sigmoid function </h2>\n",
    "<h1> $$ \\frac{1}{(1 + e^{-\\theta^\\intercal X})} $$ </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(layer_input):\n",
    "    return 1 / (1 + np.exp(-(layer_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_row(layer_output):\n",
    "    if (layer_output.ndim == 1):\n",
    "        layer_output = layer_output.reshape(1,len(layer_output)) # single dimension will lead to error\n",
    "    ones = np.ones((1,layer_output.shape[1]), dtype = float) # adding in 1s for bias weight\n",
    "    layer_output = np.append(ones, layer_output, axis=0)\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(weights, X):\n",
    "    n = len(weights) # number of layers - 1, also number of iterations for foward propagation\n",
    "    layer_outputs = []\n",
    "    layer_output = X.transpose() # makes a copy as we need to add 1 to it to form first layer output\n",
    "    for i in range(n):\n",
    "        layer_output = add_ones_row(layer_output) # adding column of 1 so bias term for each layer is included\n",
    "        layer_outputs.append(layer_output) # important to include the 1 in as it is used for calculation in back propagation\n",
    "        layer_weight = weights[i]\n",
    "        layer_input = np.matmul(layer_weight, layer_output) # calculating z, input to next layer\n",
    "        layer_output = sigmoid(layer_input) # calculating z, output of next layer\n",
    "    layer_outputs.append(layer_output)    \n",
    "    return layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(final_output, weights, y, lbd):\n",
    "    m = len(y)\n",
    "    sum_squared_weights = sum(list(map(lambda x: (x**2).sum(),weights))) # sum of squared weights\n",
    "    regularization_cost = (lbd/(2*m))*(sum_squared_weights)\n",
    "    cost = (1/m)*(-y*np.log(final_output)-(1-y)*np.log(1-final_output)).sum() + regularization_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(weights, layer_outputs, y):\n",
    "    n = len(weights) # number of layers - 1, also number of iterations for foward propagation\n",
    "    gradients = []\n",
    "    layer_error = None;\n",
    "    for i in range(n,0,-1): # from n to 1 inclusive\n",
    "        \n",
    "        if (i == n):\n",
    "            layer_error = layer_outputs[i] - y # final layer output - y\n",
    "        else:    \n",
    "            layer_error = np.matmul(weights[i].transpose(),layer_error)*layer_outputs[i]*(1-layer_outputs[i]) # vectorized implementation for layer error calculation\n",
    "            layer_error =  np.delete(layer_error, (0), axis=0) # remove first row which correspond to bias node\n",
    "        if (layer_error.ndim == 1):\n",
    "            layer_error = layer_error.reshape(1,len(layer_error)) # reshape from 1d to 2d so no error occurs\n",
    "        grad = np.matmul(layer_error,layer_outputs[i-1].transpose())\n",
    "        gradients.insert(0,grad)\n",
    "    return np.array(gradients)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, y, network_structure, iterations, lbd):\n",
    "    weights=[]\n",
    "    train_costs = []\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for i in range(len(network_structure)-1): # intializing weights\n",
    "        layer_weights = np.random.rand(network_structure[i+1],network_structure[i]+1) # +1 for bias\n",
    "        weights.append(layer_weights)\n",
    "    for i in range(iterations):\n",
    "        layer_outputs = forward_propagation(weights, X) # forward propagation to calculate output of nodes at each layer\n",
    "        iter_cost = calculate_cost(layer_outputs[-1], weights, y, lbd)\n",
    "        train_costs.append(iter_cost)\n",
    "        print(iter_cost)\n",
    "        gradients = back_propagation(weights, layer_outputs, y) # back progagation to calculate gradient of nodes at each layer\n",
    "#         weights -= (1/m)*(gradients) + (lbd/(m)*weights)\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] -= (1/m)*(gradients[i]) + (lbd/m)*(weights[i])\n",
    "    x_graph = np.arange(0,iterations,1);    \n",
    "    plt.plot(x_graph,train_costs, label='train') \n",
    "    plt.legend()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, mean, std):\n",
    "    return (X-mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, weights, threshold):\n",
    "    layer_outputs = forward_propagation(weights, X)\n",
    "    pred_result = layer_outputs[-1]\n",
    "    pred_result = (pred_result>=threshold).astype(int) # those above threshold = 1, 0 otherwise\n",
    "    return pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n",
      "268\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/logistic_diabetes.csv\")\n",
    "print(df.isnull().sum())\n",
    "print(len(df[df['Outcome']==1]))\n",
    "print(len(df[df['Outcome']==0])) # making sure data is balanced, a bit of imbalance but should be okay\n",
    "train=df.sample(frac=0.75,random_state=150) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "\n",
    "train_x = train.loc[:,train.columns != \"Outcome\"] # splitting dependent and independent variables\n",
    "test_x = test.loc[:,test.columns != \"Outcome\"]\n",
    "train_y = train['Outcome'].values\n",
    "test_y = test['Outcome'].values\n",
    "\n",
    "train_mean = train_x.mean(axis=0) # mean normalization\n",
    "train_std = train_x.std(axis=0)\n",
    "train_x = normalize(train_x,train_mean ,train_std)\n",
    "test_x = normalize(test_x,train_mean ,train_std )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.562458136724058\n",
      "0.7237623764695756\n",
      "0.6570222267004464\n",
      "0.6564650296514091\n",
      "0.6561667592350969\n",
      "0.6558747797312016\n",
      "0.6555857822154733\n",
      "0.6552995769999262\n",
      "0.6550160205575769\n",
      "0.6547349732812661\n",
      "0.6544562987135004\n",
      "0.6541798633491513\n",
      "0.6539055364532903\n",
      "0.6536331898867305\n",
      "0.6533626979387603\n",
      "0.6530939371666158\n",
      "0.652826786241261\n",
      "0.6525611257990823\n",
      "0.652296838299118\n",
      "0.6520338078854744\n",
      "0.6517719202546048\n",
      "0.6515110625271426\n",
      "0.6512511231240037\n",
      "0.6509919916464922\n",
      "0.6507335587601615\n",
      "0.6504757160821951\n",
      "0.650218356072096\n",
      "0.6499613719254783\n",
      "0.6497046574707788\n",
      "0.649448107068712\n",
      "0.6491916155143117\n",
      "0.6489350779414088\n",
      "0.6486783897294139\n",
      "0.6484214464122801\n",
      "0.6481641435895317\n",
      "0.6479063768392661\n",
      "0.6476480416330312\n",
      "0.6473890332525066\n",
      "0.6471292467079193\n",
      "0.6468685766581349\n",
      "0.6466069173323852\n",
      "0.6463441624535898\n",
      "0.6460802051632525\n",
      "0.6458149379479192\n",
      "0.6455482525671926\n",
      "0.6452800399833165\n",
      "0.6450101902923482\n",
      "0.6447385926569521\n",
      "0.6444651352408605\n",
      "0.6441897051450561\n",
      "0.6439121883457484\n",
      "0.643632469634223\n",
      "0.6433504325586645\n",
      "0.6430659593680563\n",
      "0.6427789309582875\n",
      "0.6424892268205981\n",
      "0.6421967249925212\n",
      "0.6419013020114863\n",
      "0.6416028328712687\n",
      "0.6413011909814854\n",
      "0.6409962481303504\n",
      "0.6406878744509252\n",
      "0.640375938391112\n",
      "0.6400603066876542\n",
      "0.6397408443444311\n",
      "0.6394174146153427\n",
      "0.6390898789921079\n",
      "0.6387580971973044\n",
      "0.6384219271830073\n",
      "0.6380812251353909\n",
      "0.6377358454856797\n",
      "0.637385640927846\n",
      "0.6370304624434691\n",
      "0.6366701593341831\n",
      "0.6363045792621522\n",
      "0.6359335682990226\n",
      "0.6355569709838116\n",
      "0.6351746303901991\n",
      "0.634786388203691\n",
      "0.6343920848091307\n",
      "0.6339915593890272\n",
      "0.6335846500331704\n",
      "0.633171193859996\n",
      "0.632751027150146\n",
      "0.632323985492665\n",
      "0.631889903944244\n",
      "0.6314486172019013\n",
      "0.6309999597894638\n",
      "0.6305437662581728\n",
      "0.6300798714016967\n",
      "0.6296081104857901\n",
      "0.6291283194927794\n",
      "0.6286403353810016\n",
      "0.6281439963592492\n",
      "0.627639142176208\n",
      "0.6271256144247848\n",
      "0.6266032568611482\n",
      "0.6260719157381959\n",
      "0.6255314401530775\n",
      "0.6249816824082808\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = neural_network(train_x, train_y, [train_x.shape[1],4,4,1], 100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510416666666666\n"
     ]
    }
   ],
   "source": [
    "pred_y = prediction(train_x, weights, 0.5)\n",
    "pred_y = pred_y.flatten()\n",
    "result = pred_y == train_y\n",
    "print(sum(result) / len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
